{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "{{{abstract}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-variable optimization\n",
    "\n",
    "<!-- #region -->\n",
    "\n",
    "## Optimization with functions of several variables\n",
    "\n",
    "Consider a function \\\\(f(x,y)\\\\) of two variables. You learned in\n",
    "Calculus 3 (*vector calculus*) how to search for the points \\\\((x,y)\\\\) at which\n",
    "\\\\(f\\\\) assumes its maximum and minimum value. Let's briefly recall\n",
    "this story.\n",
    "\n",
    "Recall that for a function of a single variable, critical points are those points for which the tangent line is horizontal. In the single variable case, the criteria depends instead on the *tangent plane*.\n",
    "\n",
    "Recall that the [**normal vector**](https://en.wikipedia.org/wiki/Normal_(geometry)) at the point \\\\(P=(x_0,y_0,f(x_0,y_0))\\\\) to the surface defined by \\\\(z = f(x,y)\\\\) is given by\n",
    "\\\\[\\mathbf{n}\\vert_P = \\left ( \\mathbf{i} + \\dfrac{\\partial f}{\\partial x} \\mathbf{k} \\right )_P \\times \n",
    "                        \\left ( \\mathbf{j} + \\dfrac{\\partial f}{\\partial y} \\mathbf{k} \\right )_P \n",
    "                     = \\left ( \\mathbf{k}  - \\dfrac{\\partial f}{\\partial x} \\mathbf{i}                                                                                - \\dfrac{\\partial f}{\\partial y} \\mathbf{j} \\right )_P\\\\]\n",
    "  \n",
    "Now, the **tangent plane** at \\\\(P\\\\) to the surface \\\\(z = f(x,y)\\\\) is just\n",
    "the plane orthogonal to this normal vector \\\\(\\mathbf{n}_P\\\\). Thus, the tangent plane at \n",
    "\\\\(P\\\\) is horizontal just in case this normal vector points in the \\\\(\\mathbf{k}\\\\) \n",
    "direction -- i.e. provided that\n",
    "\\\\[(\\clubsuit) \\quad \\dfrac{\\partial{f}}{\\partial{x}} \\bigg\\vert_{(x_0,y_0)} = 0 \\quad\n",
    "\\text{and} \\quad  \\dfrac{\\partial{f}}{\\partial{y}} \\bigg\\vert_{(x_0,y_0)} = 0\\\\]\n",
    "  \n",
    "Just as in the one variable case, the points \\\\((x_0,y_0)\\\\) for which the tangent plane to  the surface \\\\(P=(x_0,y_0,f(x_0,y_0))\\\\) is horizontal are the *critical points*.\n",
    "  \n",
    "So we find the critical points by simultaneously solving the euqations\\\\((\\clubsuit)\\\\).\n",
    "\n",
    "There is a *second derivative test* which gives information about the \"max/min status\" of these critical points.\n",
    "  \n",
    "To use this test, consider the matrix of second partial derivatives\n",
    "  \\\\[ M(x_0,y_0) = \\begin{pmatrix} \n",
    "        \\dfrac{\\partial^2 f}{\\partial x^2} & \\dfrac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "        \\dfrac{\\partial^2 f}{\\partial y \\partial x} & \\dfrac{\\partial^2 f}{\\partial y^2} \\\\\n",
    "      \\end{pmatrix} \\Bigg\\vert_{(x_0,y_0)}.\\\\]\n",
    "      \n",
    "For reasonable functions, the \"mixed partials\" \\\\(\\dfrac{\\partial^2 f}{\\partial x \\partial y}\\\\) and \\\\(\\dfrac{\\partial^2 f}{\\partial y \\partial x}\\\\) coincide.\n",
    "  \n",
    "The determinant of a \\\\(2 \\times 2\\\\) matrix \\\\(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\\\) is \\\\(ad - bc\\\\)).\n",
    "  \n",
    "So, the *determinant* of \\\\(M\\\\) is the expression\n",
    "  \n",
    "\\\\[D=D(x_0,y_0) = \\left(\\dfrac{\\partial^2 f}{\\partial x^2}\\cdot \\dfrac{\\partial^2 f}{\\partial y^2}\n",
    " - \\left[\\dfrac{\\partial^2 f}{\\partial x \\partial y}\\right]^2\\right) \\bigg\\vert_{(x_0,y_0)} \\\\]\n",
    "   \n",
    "   \n",
    "Suppose that \\\\((x_0,y_0)\\\\) is a critical point. \n",
    "- If \\\\(D>0\\\\) and \\\\(\\dfrac{\\partial f}{\\partial x}\\bigg\\vert_{(x_0,y_0)}<0\\\\), then \\\\(f(x,y)\\\\) has a relative maximum at \\\\((x_0,y_0)\\\\).\n",
    "- If \\\\(D>0\\\\) and \\\\(\\dfrac{\\partial f}{\\partial x}\\bigg\\vert_{(x_0,y_0)}>0\\\\), then \\\\(f(x,y)\\\\) has a relative minimum at \\\\((x_0,y_0)\\\\).\n",
    "\n",
    "- If \\\\(D<0\\\\), then \\\\(f(x,y)\\\\) has a saddle point at \\\\((x_0,y_0)\\\\).\n",
    "- If \\\\(D=0\\\\), the second derivative test is inconclusive. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange multipliers\n",
    "\n",
    "Consider a function \\\\(f(x,y)\\\\) of two variables. We are interested here in finding maximal or minimal values of \\\\(f\\\\) subject to a *constraint*. The sort of constraint we have in mind is a restriction on the possible pairs \\\\((x,y)\\\\) -- so we have a second function \\\\(g(x,y)\\\\) and we want to maximize (or minimize) \\\\(f\\\\) subject\n",
    "to the condition that \\\\(g(x,y) = c\\\\) for some fixed quantity \\\\(c\\\\).\n",
    "\n",
    "We introduce a \"new\" function -- now of *three* variables - known as the **Lagrangian**. It is given by the formula\n",
    "\\\\[F(x,y,\\lambda) = f(x,y) - \\lambda \\cdot (c-g(x,y))\\\\]\n",
    "\n",
    "We can calculate the *partial derivatives* of this Lagrangian; they are:\n",
    "\n",
    "\\\\[\\dfrac{\\partial F}{\\partial x} = \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x}\\\\]\n",
    "\n",
    "\\\\[\\dfrac{\\partial F}{\\partial y} = \\dfrac{\\partial f}{\\partial y} - \\lambda\\dfrac{\\partial g}{\\partial y}\\\\]\n",
    "\n",
    "\\\\[\\dfrac{\\partial F}{\\partial \\lambda} = c-g(x,y)\\\\]\n",
    "\n",
    "If we seek critical points of the Lagrangian, we find that \n",
    "\\\\[0 = \\dfrac{\\partial F}{\\partial x} = \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x}\\\\]\n",
    "and similarly for \\\\(y\\\\), so that\n",
    "\\\\[ \\dfrac{\\partial f}{\\partial x} = \\lambda \\dfrac{\\partial g}{\\partial x} \\quad \\text{and}\\quad\n",
    "\\dfrac{\\partial f}{\\partial x} = \\lambda \\dfrac{\\partial g}{\\partial x}\\\\]\n",
    "i.e.\n",
    "\\\\[ \\left (\\dfrac{\\partial f}{\\partial x} \\mathbf{i} + \\dfrac{\\partial f}{\\partial y} \\mathbf{j} \\right)\n",
    "= \\lambda \\left (\\dfrac{\\partial g}{\\partial x} \\mathbf{i} + \\dfrac{\\partial g}{\\partial y} \\mathbf{j} \\right)\\\\]\n",
    "\n",
    "(Recall that \\\\(\\dfrac{\\partial f}{\\partial x} \\mathbf{i} + \\dfrac{\\partial f}{\\partial y} \\mathbf{j}\\\\) is the\n",
    "*gradient* \\\\(\\nabla f\\\\) of \\\\(f\\\\)).\n",
    "\n",
    "Moreover, we find that\n",
    "\\\\[0 = \\dfrac{\\partial F}{\\partial \\lambda} = c - g(x,y).\\\\]\n",
    "\n",
    "Summarizing, the condition that \\\\((x_0,y_0,\\lambda_0)\\\\) is a critical point of \\\\(F\\\\) is equivalent to two requirements: \n",
    "- \\\\((x_0,y_0)\\\\) must be on the level curve \\\\(g(x,y) = c\\\\), and\n",
    "- the gradient vectors must satisfy \\\\(\\nabla f \\vert_{(x_0,y_0)} = \\lambda_0 \\nabla g \\vert_{(x_0,y_0)}\\\\).\n",
    "\n",
    "The crucial point is: optimal values for \\\\(f\\\\) along the level curve \\\\(g(x,y) = c\\\\) will be found among the critical points of \\\\(F\\\\). \n",
    "\n",
    "Indeed. suppose \\\\((x_0,y_0)\\\\)\n",
    "is a point on the level curve at which \\\\(f\\\\) takes its max (or min) value (on the level surface).\n",
    "We need to argue that the gradient vector \\\\(\\nabla f \\vert_{(x_0,y_0)}\\\\) is \"parallel\" to the gradient\n",
    "vector \\\\(\\nabla g \\vert_{(x_0,y_0)}\\\\).\n",
    "\n",
    "More precisely, we can write \\\\(\\nabla g \\vert_{(x_0,y_0)} = \\mathbf{v} + \\mu \\nabla f \\vert_{(x_0,y_0)}\\\\)\n",
    "for a vector \\\\(\\mathbf{v}\\\\) perpendicular to \\\\(\\nabla f \\vert_{(x_0,y_0)}\\\\) (and for some scalar \\\\(\\mu\\\\)).\n",
    "And we must argue that \\\\(\\mathbf{v}\\\\) is zero.\n",
    "\n",
    "But if \\\\(\\mathbf{v}\\\\) is non-zero, then walking along the level curve \\\\(g(x,y) = c\\\\) \"in the direction of \\\\(\\mathbf{v}\\\\)\" the  \n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
